{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andcore20/Fifteen_Hundred_Curso_Dados/blob/main/Desafio_Final_Testes_com_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsps8iCPKd73"
      },
      "source": [
        "# PYSPARK ON COLAB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYLfV_61JkpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd5b435-52c2-4878-db31-1a87629cb906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: https://cloud.r-project.org/bin/linux/ubuntu/jammy-cran40/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bpLKpf6KqH-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVNuN962KsKE"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZjIjLVlKwpF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "  .master('local[*]')\\\n",
        "  .appName(\"Iniciando com Spark\")\\\n",
        "  .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCVZPc9fXjoM"
      },
      "source": [
        "# Importando dados com pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Vh97UZXi2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869506d8-168a-4188-e3f5-e8742facaaf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_bank = '/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/bank-full.csv'\n",
        "path_bank_ad = '/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/bank-additional-full.csv'\n",
        "\n",
        "bank = pd.read_csv(path_bank, sep=';')\n",
        "bank_ad = pd.read_csv(path_bank_ad, sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30o9rk3PYIB6"
      },
      "source": [
        "# Tratamento Perfil de Cliente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsRkAqoitkn"
      },
      "source": [
        "## Filtro poutcome = 'success' ou y = 'yes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsp3u_jNdjnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd684657-14d0-4428-c84f-27db4c8ece53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---+--------+-------------------+-------+-------+----+-----------+---+\n",
            "|         job|age| marital|          education|default|housing|loan|   poutcome|  y|\n",
            "+------------+---+--------+-------------------+-------+-------+----+-----------+---+\n",
            "| blue-collar| 41|divorced|           basic.4y|unknown|    yes|  no|nonexistent|yes|\n",
            "|entrepreneur| 49| married|  university.degree|unknown|    yes|  no|nonexistent|yes|\n",
            "|  technician| 49| married|           basic.9y|     no|     no|  no|nonexistent|yes|\n",
            "|  technician| 41| married|professional.course|unknown|    yes|  no|nonexistent|yes|\n",
            "| blue-collar| 45| married|           basic.9y|unknown|    yes|  no|nonexistent|yes|\n",
            "| blue-collar| 42| married|           basic.9y|     no|    yes| yes|nonexistent|yes|\n",
            "|   housemaid| 39| married|           basic.9y|     no|    yes|  no|nonexistent|yes|\n",
            "|     unknown| 28|  single|            unknown|unknown|    yes| yes|nonexistent|yes|\n",
            "|    services| 44| married|        high.school|     no|    yes|  no|nonexistent|yes|\n",
            "|  technician| 42| married|professional.course|     no|     no|  no|nonexistent|yes|\n",
            "|  management| 42| married|  university.degree|     no|     no|  no|nonexistent|yes|\n",
            "|    services| 39| married|        high.school|unknown|    yes|  no|nonexistent|yes|\n",
            "| blue-collar| 42| married|        high.school|     no|     no| yes|nonexistent|yes|\n",
            "|   housemaid| 45| married|professional.course|unknown|    yes|  no|nonexistent|yes|\n",
            "|      admin.| 31|  single|  university.degree|     no|     no|  no|nonexistent|yes|\n",
            "|     student| 28|  single|           basic.9y|unknown|    yes| yes|nonexistent|yes|\n",
            "|entrepreneur| 41| married|  university.degree|     no|    yes|  no|nonexistent|yes|\n",
            "| blue-collar| 37|  single|           basic.9y|unknown|    yes|  no|nonexistent|yes|\n",
            "| blue-collar| 44| married|           basic.4y|     no|    yes|  no|nonexistent|yes|\n",
            "|entrepreneur| 44| married|           basic.4y|unknown|     no|  no|nonexistent|yes|\n",
            "+------------+---+--------+-------------------+-------+-------+----+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Crie uma sessão do Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Converta os DataFrames pandas em DataFrames Spark\n",
        "bank_ad_spark = spark.createDataFrame(bank_ad)\n",
        "bank_spark = spark.createDataFrame(bank)\n",
        "bank_spark = bank_spark[['job', 'age', 'marital', 'education', 'default', 'housing', 'loan', 'poutcome', 'y']]\n",
        "bank_ad_spark = bank_ad_spark[['job', 'age', 'marital', 'education', 'default', 'housing', 'loan', 'poutcome', 'y']]\n",
        "\n",
        "# Defina as condições\n",
        "cond1 = (col(\"poutcome\") == \"success\") | (col(\"y\") == \"yes\")\n",
        "cond2 = (col(\"poutcome\") == \"success\") | (col(\"y\") == \"yes\")\n",
        "\n",
        "# Aplique as condições e faça a concatenação dos DataFrames\n",
        "bank_final = bank_ad_spark.filter(cond1).union(bank_spark.filter(cond2))\n",
        "\n",
        "# Exiba o DataFrame resultante\n",
        "bank_final.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QFyH9Cnchfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0af114e-77dd-4602-c6aa-ffdd62c309a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10941"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "bank_final.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IWJZq1A8gU6"
      },
      "source": [
        "## Ajustando alguns valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyp83RYNp286"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Exibir valores únicos de cada coluna\n",
        "#for column in bank_final.columns:\n",
        "#    unique_values = bank_final.select(column).distinct().collect()\n",
        "#    print(f\"Valores únicos da coluna '{column}':\")\n",
        "#    for value in unique_values:\n",
        "#        print(value[0])\n",
        "#    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRnqNEvg8mo4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Tratar valores 'unknown', 'other' e 'nonexistent' como 'failure'\n",
        "#bank_final = bank_final.withColumn(\"poutcome\", when(bank_final[\"poutcome\"].isin([\"unknown\", \"other\", \"nonexistent\"]), \"failure\").otherwise(bank_final[\"poutcome\"]))\n",
        "#bank_final.select(\"poutcome\").distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWZkx1NEi3EL"
      },
      "source": [
        "## Ajustando coluna education"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_QGpE2Pipt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63124fa7-04cc-472a-cf97-0029871b2a58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10941"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Dicionário de substituição\n",
        "substitution_dict = {\n",
        "    'basic.4y': 'basic',\n",
        "    'basic.6y': 'basic',\n",
        "    'basic.9y': 'basic',\n",
        "    'high.school': 'secondary',\n",
        "    'university.degree': 'tertiary',\n",
        "    'professional.course': 'tertiary'\n",
        "}\n",
        "\n",
        "# Substituir os valores na coluna \"education\" usando a função when() e col()\n",
        "bank_final = bank_final.withColumn(\n",
        "    'education',\n",
        "    when(col('education').isin(list(substitution_dict.keys())),\n",
        "         col('education')).otherwise(col('education'))\n",
        ")\n",
        "\n",
        "# Substituir os valores na coluna \"education\" usando a função replace()\n",
        "for old_value, new_value in substitution_dict.items():\n",
        "    bank_final = bank_final.replace(old_value, new_value, 'education')\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "bank_final.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVesmXSKhiZW"
      },
      "source": [
        "## Coluna faixa_idades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHg0NnzcBs6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2eacfd-9e72-43f8-dd81-b88c0a6eca1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---+--------+---------+-------+-------+----+-----------+---+------------+\n",
            "|         job|age| marital|education|default|housing|loan|   poutcome|  y|faixa_idades|\n",
            "+------------+---+--------+---------+-------+-------+----+-----------+---+------------+\n",
            "| blue-collar| 41|divorced|    basic|unknown|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "|entrepreneur| 49| married| tertiary|unknown|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "|  technician| 49| married|    basic|     no|     no|  no|nonexistent|yes|     40 - 59|\n",
            "|  technician| 41| married| tertiary|unknown|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "| blue-collar| 45| married|    basic|unknown|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "| blue-collar| 42| married|    basic|     no|    yes| yes|nonexistent|yes|     40 - 59|\n",
            "|   housemaid| 39| married|    basic|     no|    yes|  no|nonexistent|yes|     26 - 39|\n",
            "|     unknown| 28|  single|  unknown|unknown|    yes| yes|nonexistent|yes|     26 - 39|\n",
            "|    services| 44| married|secondary|     no|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "|  technician| 42| married| tertiary|     no|     no|  no|nonexistent|yes|     40 - 59|\n",
            "|  management| 42| married| tertiary|     no|     no|  no|nonexistent|yes|     40 - 59|\n",
            "|    services| 39| married|secondary|unknown|    yes|  no|nonexistent|yes|     26 - 39|\n",
            "| blue-collar| 42| married|secondary|     no|     no| yes|nonexistent|yes|     40 - 59|\n",
            "|   housemaid| 45| married| tertiary|unknown|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "|      admin.| 31|  single| tertiary|     no|     no|  no|nonexistent|yes|     26 - 39|\n",
            "|     student| 28|  single|    basic|unknown|    yes| yes|nonexistent|yes|     26 - 39|\n",
            "|entrepreneur| 41| married| tertiary|     no|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "| blue-collar| 37|  single|    basic|unknown|    yes|  no|nonexistent|yes|     26 - 39|\n",
            "| blue-collar| 44| married|    basic|     no|    yes|  no|nonexistent|yes|     40 - 59|\n",
            "|entrepreneur| 44| married|    basic|unknown|     no|  no|nonexistent|yes|     40 - 59|\n",
            "+------------+---+--------+---------+-------+-------+----+-----------+---+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Defina as condições para cada faixa de idade\n",
        "conditions = [\n",
        "    (col(\"age\").between(17, 25)),\n",
        "    (col(\"age\").between(26, 39)),\n",
        "    (col(\"age\").between(40, 59)),\n",
        "    (col(\"age\").between(60, 98))\n",
        "]\n",
        "\n",
        "# Defina os valores correspondentes para cada faixa de idade\n",
        "values = ['17 - 25', '26 - 39', '40 - 59', '60 - 98']\n",
        "\n",
        "# Use a função 'when' para atribuir os valores corretos com base nas condições\n",
        "bank_final = bank_final.withColumn(\"faixa_idades\", when(conditions[0], values[0])\n",
        "                                                        .when(conditions[1], values[1])\n",
        "                                                        .when(conditions[2], values[2])\n",
        "                                                        .when(conditions[3], values[3])\n",
        "                                                        .otherwise(None))\n",
        "\n",
        "# Mostre o DataFrame resultante\n",
        "bank_final.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_final.filter(col('faixa_idades') == '17 - 25').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zDu4n7tM5G",
        "outputId": "c1d22b99-8429-44dc-c27f-78ff5914e9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "726"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_final.filter((bank_final['faixa_idades'] == '17 - 25') & (bank_final['y'] == 'yes') & (bank_final['poutcome'] == 'success')).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3f9eqKltZWw",
        "outputId": "2fe1f749-934d-4b55-c489-e27516807828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27TVlaueL6po"
      },
      "source": [
        "## Agrupamento faixa idade micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od-LOGLFL9sB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Defina as faixas de idades desejadas\n",
        "idade_inicial = 17\n",
        "idade_final = 98\n",
        "intervalo_idades = 3\n",
        "\n",
        "# Crie uma lista de tuplas para armazenar as condições e os valores\n",
        "faixas_idade = []\n",
        "\n",
        "# Use um loop for para criar as condições e os valores para todas as faixas de idades\n",
        "for i in range(idade_inicial, idade_final + 1, intervalo_idades):\n",
        "    faixa_idade = f\"{i} - {i + intervalo_idades - 1}\"\n",
        "    condition = col(\"age\").between(i, i + intervalo_idades - 1)\n",
        "    faixas_idade.append((condition, faixa_idade))\n",
        "\n",
        "# Crie uma coluna inicializada com None\n",
        "faixa_idades_micro_col = None\n",
        "\n",
        "# Use a função 'when' com a lista de tuplas de condições e valores para criar a coluna 'faixa_idades_micro'\n",
        "for condition, value in faixas_idade:\n",
        "    if faixa_idades_micro_col is None:\n",
        "        faixa_idades_micro_col = when(condition, value)\n",
        "    else:\n",
        "        faixa_idades_micro_col = faixa_idades_micro_col.when(condition, value)\n",
        "\n",
        "# Atribua o valor padrão None usando a função otherwise\n",
        "faixa_idades_micro_col = faixa_idades_micro_col.otherwise(None)\n",
        "\n",
        "# Adicione a coluna 'faixa_idades_micro' ao DataFrame\n",
        "bank_final = bank_final.withColumn(\"faixa_idades_micro\", faixa_idades_micro_col)\n",
        "\n",
        "# Mostre o DataFrame resultante\n",
        "bank_final.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela Perfil_Geral"
      ],
      "metadata": {
        "id": "YTice_6B2pKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perfil_geral = bank_final"
      ],
      "metadata": {
        "id": "Yg4fZFYk2nCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela perfil_faixa_idades"
      ],
      "metadata": {
        "id": "Q7J3eciF3FHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "\n",
        "# Agrupamento por faixa de idade e contagem\n",
        "grouped_df = bank_final.groupBy('faixa_idades').agg(\n",
        "    F.count(F.lit(1)).alias('count'),\n",
        "    F.round((F.sum(F.when(F.col('poutcome') == 'success', 1).otherwise(0)) / F.count(F.lit(1))), 2).alias('conversão_anterior'),\n",
        "    F.round((F.sum(F.when(F.col('y') == 'yes', 1).otherwise(0)) / F.count(F.lit(1))), 2).alias('conversão_atual'),\n",
        "    F.round((F.sum(F.when(F.col('poutcome') == 'failure', 1).otherwise(0)) / F.count(F.lit(1))), 2).alias('churn_anterior'),\n",
        "    F.round((F.sum(F.when(F.col('y') == 'yes', 1).otherwise(0)) / F.count(F.lit(1))), 2).alias('churn_atual')\n",
        ")\n",
        "\n",
        "# Cálculo da média da coluna age\n",
        "average_age = bank_final.select(F.avg('age')).collect()[0][0]\n",
        "average_age = int(average_age + 0.5)  # Arredonda para cima caso .5 ou maior\n",
        "\n",
        "# Valores que aparecem com maior frequência em cada coluna\n",
        "most_frequent_job = bank_final.groupBy('job').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('job').collect()[0][0]\n",
        "most_frequent_marital = bank_final.groupBy('marital').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('marital').collect()[0][0]\n",
        "most_frequent_education = bank_final.groupBy('education').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('education').collect()[0][0]\n",
        "most_frequent_default = bank_final.groupBy('default').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('default').collect()[0][0]\n",
        "most_frequent_housing = bank_final.groupBy('housing').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('housing').collect()[0][0]\n",
        "most_frequent_loan = bank_final.groupBy('loan').agg(F.count(F.lit(1)).alias('count')).orderBy(F.desc('count')).select('loan').collect()[0][0]\n",
        "\n",
        "# Criação do DataFrame com os resultados\n",
        "teste = grouped_df.withColumn('age', F.lit(average_age)) \\\n",
        "                     .withColumn('job', F.lit(most_frequent_job)) \\\n",
        "                     .withColumn('marital', F.lit(most_frequent_marital)) \\\n",
        "                     .withColumn('education', F.lit(most_frequent_education)) \\\n",
        "                     .withColumn('default', F.lit(most_frequent_default)) \\\n",
        "                     .withColumn('housing', F.lit(most_frequent_housing)) \\\n",
        "                     .withColumn('loan', F.lit(most_frequent_loan))\n",
        "\n",
        "teste.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDTwr7Lh3EeX",
        "outputId": "9780d425-9d80-4a73-9cda-69de6e80bdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+------------------+---------------+--------------+-----------+---+------+-------+---------+-------+-------+----+\n",
            "|faixa_idades|count|conversão_anterior|conversão_atual|churn_anterior|churn_atual|age|   job|marital|education|default|housing|loan|\n",
            "+------------+-----+------------------+---------------+--------------+-----------+---+------+-------+---------+-------+-------+----+\n",
            "|     60 - 98| 1185|              0.37|            0.9|          0.12|        0.9| 41|admin.|married| tertiary|     no|     no|  no|\n",
            "|     17 - 25|  726|              0.27|           0.92|          0.11|       0.92| 41|admin.|married| tertiary|     no|     no|  no|\n",
            "|     26 - 39| 5326|              0.25|            0.9|          0.11|        0.9| 41|admin.|married| tertiary|     no|     no|  no|\n",
            "|     40 - 59| 3704|              0.25|           0.91|          0.11|       0.91| 41|admin.|married| tertiary|     no|     no|  no|\n",
            "+------------+-----+------------------+---------------+--------------+-----------+---+------+-------+---------+-------+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, round, avg\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc, rank\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Agrupar por faixa_idades e contar a quantidade de vezes que aparece cada faixa\n",
        "grouped_df = bank_final.groupBy('faixa_idades').agg(count('faixa_idades').alias('count'))\n",
        "\n",
        "# Calcular a taxa de conversão por faixa de idades\n",
        "conversao_df = bank_final.filter((bank_final['poutcome'] == 'success') & (bank_final['y'] == 'yes')) \\\n",
        "    .groupBy('faixa_idades').agg((count('faixa_idades') / bank_final.count()).alias('conversão'))\n",
        "\n",
        "# Obter o valor mais frequente de 'job' para cada faixa de idades\n",
        "job_df = bank_final.groupBy('faixa_idades', 'job').agg(count('job').alias('job_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('job_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'job')\n",
        "\n",
        "# Calcular a média da coluna 'age' e arredondar para inteiro para cada faixa de idades\n",
        "age_df = bank_final.groupBy('faixa_idades').agg(round(avg('age')).cast('integer').alias('age'))\n",
        "\n",
        "# Obter o valor mais frequente de 'marital' para cada faixa de idades\n",
        "marital_df = bank_final.groupBy('faixa_idades', 'marital').agg(count('marital').alias('marital_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('marital_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'marital')\n",
        "\n",
        "# Obter o valor mais frequente de 'education' para cada faixa de idades\n",
        "education_df = bank_final.groupBy('faixa_idades', 'education').agg(count('education').alias('education_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('education_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'education')\n",
        "\n",
        "# Obter o valor mais frequente de 'default' para cada faixa de idades\n",
        "default_df = bank_final.groupBy('faixa_idades', 'default').agg(count('default').alias('default_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('default_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'default')\n",
        "\n",
        "# Obter o valor mais frequente de 'housing' para cada faixa de idades\n",
        "housing_df = bank_final.groupBy('faixa_idades', 'housing').agg(count('housing').alias('housing_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('housing_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'housing')\n",
        "\n",
        "# Obter o valor mais frequente de 'loan' para cada faixa de idades\n",
        "loan_df = bank_final.groupBy('faixa_idades', 'loan').agg(count('loan').alias('loan_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('loan_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'loan')\n",
        "\n",
        "# Juntar todos os dataframes em um único dataframe\n",
        "result_df = grouped_df.join(conversao_df, 'faixa_idades', 'left') \\\n",
        "    .join(job_df, 'faixa_idades', 'left') \\\n",
        "    .join(age_df, 'faixa_idades', 'left') \\\n",
        "    .join(marital_df, 'faixa_idades', 'left') \\\n",
        "    .join(education_df, 'faixa_idades', 'left') \\\n",
        "    .join(default_df, 'faixa_idades', 'left') \\\n",
        "    .join(housing_df, 'faixa_idades', 'left') \\\n",
        "    .join(loan_df, 'faixa_idades', 'left')\n",
        "\n",
        "# Ordenar o DataFrame resultante na ordem '17 - 25', '26 - 39', '40 - 59', '60 - 98'\n",
        "result_df = result_df.orderBy(col(\"faixa_idades\").asc())\n",
        "\n",
        "# Multiplicar a coluna 'nome_da_coluna' por 100\n",
        "result_df = result_df.withColumn('conversão', col('conversão') * 100).drop('conversao')\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "_y1mG6gXIHZk",
        "outputId": "2b0ded3b-091a-4865-8127-29ce7c349430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c22ac45dcf8d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Agrupar por faixa_idades e contar a quantidade de vezes que aparece cada faixa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrouped_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbank_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'faixa_idades'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'faixa_idades'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Calcular a taxa de conversão por faixa de idades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bank_final' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.filter(col('faixa_idades') == '17 - 25').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "-ElByel1s9An",
        "outputId": "37e07405-bf29-4a91-d256-c98034716d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5964f4496d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'faixa_idades'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'17 - 25'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, round, avg, col, when\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc, rank\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Recreate a coluna 'faixa_idades' usando a coluna 'age'\n",
        "bank_final = bank_final.withColumn('faixa_idades',\n",
        "                        when((col('age') >= 17) & (col('age') <= 25), '17 - 25')\n",
        "                       .when((col('age') >= 26) & (col('age') <= 39), '26 - 39')\n",
        "                       .when((col('age') >= 40) & (col('age') <= 59), '40 - 59')\n",
        "                       .when((col('age') >= 60) & (col('age') <= 98), '60 - 98')\n",
        "                       .otherwise('Outra'))\n",
        "\n",
        "# Agrupar por faixa_idades e contar a quantidade de vezes que aparece cada faixa\n",
        "grouped_df = bank_final.groupBy('faixa_idades').agg(count('faixa_idades').alias('count'))\n",
        "\n",
        "# Calcular a taxa de conversão por faixa de idades\n",
        "conversao_df = bank_final.filter((bank_final['poutcome'] == 'success') & (bank_final['y'] == 'yes')) \\\n",
        "    .groupBy('faixa_idades').agg((count('faixa_idades') / grouped_df.select('count').collect()[0][0]).alias('conversão'))\n",
        "\n",
        "# Obter o valor mais frequente de 'job' para cada faixa de idades\n",
        "job_df = bank_final.groupBy('faixa_idades', 'job').agg(count('job').alias('job_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('job_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'job')\n",
        "\n",
        "# Calcular a média da coluna 'age' e arredondar para inteiro para cada faixa de idades\n",
        "age_df = bank_final.groupBy('faixa_idades').agg(round(avg('age')).cast('integer').alias('age'))\n",
        "\n",
        "# Obter o valor mais frequente de 'marital' para cada faixa de idades\n",
        "marital_df = bank_final.groupBy('faixa_idades', 'marital').agg(count('marital').alias('marital_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('marital_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'marital')\n",
        "\n",
        "# Obter o valor mais frequente de 'education' para cada faixa de idades\n",
        "education_df = bank_final.groupBy('faixa_idades', 'education').agg(count('education').alias('education_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('education_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'education')\n",
        "\n",
        "# Obter o valor mais frequente de 'default' para cada faixa de idades\n",
        "default_df = bank_final.groupBy('faixa_idades', 'default').agg(count('default').alias('default_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('default_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'default')\n",
        "\n",
        "# Obter o valor mais frequente de 'housing' para cada faixa de idades\n",
        "housing_df = bank_final.groupBy('faixa_idades', 'housing').agg(count('housing').alias('housing_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('housing_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'housing')\n",
        "\n",
        "# Obter o valor mais frequente de 'loan' para cada faixa de idades\n",
        "loan_df = bank_final.groupBy('faixa_idades', 'loan').agg(count('loan').alias('loan_count')) \\\n",
        "    .withColumn('rank', rank().over(Window.partitionBy('faixa_idades').orderBy(desc('loan_count')))) \\\n",
        "    .filter(col('rank') == 1).select('faixa_idades', 'loan')\n",
        "\n",
        "# Juntar todos os dataframes em um único dataframe\n",
        "perfil_faixa_idades = grouped_df.join(conversao_df, 'faixa_idades', 'left') \\\n",
        "    .join(job_df, 'faixa_idades', 'left') \\\n",
        "    .join(age_df, 'faixa_idades', 'left') \\\n",
        "    .join(marital_df, 'faixa_idades', 'left') \\\n",
        "    .join(education_df, 'faixa_idades', 'left') \\\n",
        "    .join(default_df, 'faixa_idades', 'left') \\\n",
        "    .join(housing_df, 'faixa_idades', 'left') \\\n",
        "    .join(loan_df, 'faixa_idades', 'left')\n",
        "\n",
        "# Ordenar o DataFrame resultante na ordem '17 - 25', '26 - 39', '40 - 59', '60 - 98'\n",
        "perfil_faixa_idades = perfil_faixa_idades.orderBy(col(\"faixa_idades\").asc())\n",
        "\n",
        "# Multiplicar a coluna 'conversão' por 100\n",
        "perfil_faixa_idades = perfil_faixa_idades.withColumn('conversão', col('conversão') * 100)\n",
        "\n",
        "\n",
        "perfil_faixa_idades.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "hUtOECkafQqy",
        "outputId": "e78f8d70-39c8-48d3-d988-0001dbdd6dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fa8d75a8d978>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Recreate a coluna 'faixa_idades' usando a coluna 'age'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m bank_final = bank_final.withColumn('faixa_idades', \n\u001b[0m\u001b[1;32m     11\u001b[0m                         \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'17 - 25'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                        \u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'26 - 39'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bank_final' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela perfil_job"
      ],
      "metadata": {
        "id": "vLvG_Zv7APbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "# Agrupar por job e contar a quantidade de vezes que cada job aparece\n",
        "grouped_df = bank_final.groupBy('job').agg(count('job').alias('count'))\n",
        "\n",
        "# Calcular a conversão por job\n",
        "conversao_df = bank_final.filter((bank_final['poutcome'] == 'success') & (bank_final['y'] == 'yes')) \\\n",
        "    .groupBy('job').agg((count('job') / bank_final.count()).alias('conversão'))\n",
        "\n",
        "# Juntar os dois dataframes em um único dataframe\n",
        "perfil_job = grouped_df.join(conversao_df, 'job', 'left').orderBy(col(\"count\").desc())\n",
        "\n",
        "perfil_job = perfil_job.withColumn('conversão', col('conversão') * 100)\n",
        "\n",
        "# Exibir a tabela resultante\n",
        "perfil_job.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmAmveRFAP7z",
        "outputId": "40dcc129-3a77-4a5a-92c3-d38bb2f1e7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+-------------------+\n",
            "|          job|count|          conversão|\n",
            "+-------------+-----+-------------------+\n",
            "|       admin.| 2207| 3.7290924047162046|\n",
            "|   management| 1779|  3.034457545014167|\n",
            "|   technician| 1749| 2.5317612649666392|\n",
            "|  blue-collar| 1469| 1.3161502604880724|\n",
            "|      retired| 1040| 2.2118636322091216|\n",
            "|     services|  746| 0.9231331688145508|\n",
            "|      student|  603| 1.3344301252170734|\n",
            "|   unemployed|  381| 0.8317338451695457|\n",
            "|self-employed|  369|0.47527648295402614|\n",
            "| entrepreneur|  274|0.18279864729001005|\n",
            "|    housemaid|  238| 0.4021570240380221|\n",
            "|      unknown|   86|0.13709898546750754|\n",
            "+-------------+-----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tabela perfil_marital"
      ],
      "metadata": {
        "id": "7jALrysjFf22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "# Agrupar por marital e contar a quantidade de vezes que cada marital aparece\n",
        "grouped_df = bank_final.groupBy('marital').agg(count('marital').alias('count'))\n",
        "\n",
        "# Calcular a conversão por marital\n",
        "conversao_df = bank_final.filter((bank_final['poutcome'] == 'success') & (bank_final['y'] == 'yes')) \\\n",
        "    .groupBy('marital').agg((count('marital') / bank_final.count()).alias('conversão'))\n",
        "\n",
        "# Juntar os dois dataframes em um único dataframe\n",
        "perfil_marital = grouped_df.join(conversao_df, 'marital', 'left').orderBy(col(\"count\").desc())\n",
        "\n",
        "perfil_marital = perfil_marital.withColumn('conversão', col('conversão') * 100)\n",
        "\n",
        "# Exibir a tabela resultante\n",
        "perfil_marital.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hE5BjQzTnIH",
        "outputId": "e0662bf6-6372-42e6-ce47-ac09ce08e275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------------------+\n",
            "| marital|count|           conversão|\n",
            "+--------+-----+--------------------+\n",
            "| married| 5830|  0.0928617128233251|\n",
            "|  single| 3904|0.060963348871218355|\n",
            "|divorced| 1193|0.017000274197970934|\n",
            "| unknown|   14|2.741979709350150...|\n",
            "+--------+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela perfil_education"
      ],
      "metadata": {
        "id": "KTVzoL2yFbZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "# Agrupar por education e contar a quantidade de vezes que cada education aparece\n",
        "grouped_df = bank_final.groupBy('education').agg(count('education').alias('count'))\n",
        "\n",
        "# Calcular a conversão por education\n",
        "conversao_df = bank_final.filter((bank_final['poutcome'] == 'success') & (bank_final['y'] == 'yes')) \\\n",
        "    .groupBy('education').agg((count('education') / bank_final.count()).alias('conversão'))\n",
        "\n",
        "# Juntar os dois dataframes em um único dataframe\n",
        "perfil_education = grouped_df.join(conversao_df, 'education', 'left').orderBy(col(\"count\").desc())\n",
        "\n",
        "perfil_education = perfil_education.withColumn('conversão', col('conversão') * 100)\n",
        "\n",
        "# Exibir a tabela resultante\n",
        "perfil_education.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eedqq8B2T8Ch",
        "outputId": "05fc9114-9f89-4374-b5ce-c67605378b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "| education|count|           conversão|\n",
            "+----------+-----+--------------------+\n",
            "|  tertiary| 4712| 0.08107120007311946|\n",
            "| secondary| 3833|0.056393382688968104|\n",
            "|     basic| 1200|0.015172287725070835|\n",
            "|   primary|  643|0.007403345215245407|\n",
            "|   unknown|  549|0.010967918837400604|\n",
            "|illiterate|    4|9.139932364500503E-5|\n",
            "+----------+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce9rYlC-mq4m"
      },
      "source": [
        "## teste k-means chat bing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBdQ7ZUCmv5P"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Criando uma sessão Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Selecionando as colunas relevantes do DataFrame\n",
        "data = bank_final.select(\"age\", \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\", \"y\")\n",
        "\n",
        "# Convertendo a coluna \"age\" para tipo inteiro\n",
        "data = data.withColumn(\"age\", data[\"age\"].cast(\"integer\"))\n",
        "\n",
        "# Removendo quaisquer linhas com valores nulos\n",
        "data = data.dropna()\n",
        "\n",
        "# Criando um vetor de recursos a partir das colunas relevantes\n",
        "assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Aplicando o algoritmo K-means para definir 3 clusters\n",
        "kmeans = KMeans(k=3, seed=123)\n",
        "model = kmeans.fit(data)\n",
        "\n",
        "# Obtendo as previsões dos clusters\n",
        "predictions = model.transform(data)\n",
        "\n",
        "# Exibindo os resultados\n",
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkz1bzJztHlq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, desc, first, count\n",
        "\n",
        "# Inicialize o SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Carregue a base de dados 'bank_final' para um DataFrame\n",
        "bank_final = spark.read.csv('caminho_do_arquivo/bank_final.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Tabela 1: Agrupe por faixa_idades_frequencia, job com maior frequência e contagem\n",
        "tabela1 = bank_final.groupBy(\"faixa_idades_frequencia\") \\\n",
        "                    .agg(first(\"job\").alias(\"job_maior_frequencia\"),\n",
        "                         count(\"job\").alias(\"quantidade\")) \\\n",
        "                    .orderBy(\"faixa_idades_frequencia\")\n",
        "\n",
        "# Tabela 2: marital com maior frequência e contagem\n",
        "tabela2 = bank_final.groupBy(\"marital\") \\\n",
        "                    .agg(first(\"marital\").alias(\"marital_maior_frequencia\"),\n",
        "                         count(\"marital\").alias(\"quantidade\")) \\\n",
        "                    .orderBy(\"marital\")\n",
        "\n",
        "# Tabela 3: education com maior frequência e contagem\n",
        "tabela3 = bank_final.groupBy(\"education\") \\\n",
        "                    .agg(first(\"education\").alias(\"education_maior_frequencia\"),\n",
        "                         count(\"education\").alias(\"quantidade\")) \\\n",
        "                    .orderBy(\"education\")\n",
        "\n",
        "# Mostre as tabelas resultantes\n",
        "tabela1.show()\n",
        "tabela2.show()\n",
        "tabela3.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando"
      ],
      "metadata": {
        "id": "5hmZSCnYLY9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perfil_geral.to_csv('/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_geral.csv',index=False)\n",
        "result_df.to_csv('/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_faixa_idades.csv',index=False)\n",
        "perfil_job.to_csv('/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_job.csv',index=False)\n",
        "perfil_marital.to_csv('/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_marital.csv',index=False)\n",
        "perfil_education.to_csv('/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_education.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "zJe0C8spLckd",
        "outputId": "56755006-ccdc-4c6a-f322-b84e9626cd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-873752cc150a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperfil_geral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_geral.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_faixa_idades.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mperfil_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_job.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mperfil_marital\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_marital.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mperfil_education\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/17.UNI1500th/Projetos em grupo AWS UNI1500/Desafio final/Analise Perfil Cliente/perfil_education.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he9AT8dKRlM2"
      },
      "source": [
        "# Analise evolução da campanha"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Crie uma sessão do Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "bank_ad_campanha = spark.createDataFrame(bank_ad)\n",
        "bank_ad_campanha = bank_ad_campanha.drop('age','job','marital','education','default','housing','loan')"
      ],
      "metadata": {
        "id": "G-itcS9mDdFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5bb53a-4c79-4357-acbc-a080da6d13a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_ad_campanha.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DbUn9LiDvhI",
        "outputId": "69f77c79-d79a-4e97-addc-d04e06746944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|      55|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     222|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     137|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     293|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     146|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     174|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     312|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     440|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     353|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     195|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela campanha_geral"
      ],
      "metadata": {
        "id": "eOBMVQ5hmJ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "campanha_geral = bank_ad_campanha"
      ],
      "metadata": {
        "id": "jKIYw1E4mOXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela campanha_month_previous_campaing"
      ],
      "metadata": {
        "id": "Q6IysEgDmOxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela campanha_month_yes"
      ],
      "metadata": {
        "id": "u99O6txGnLSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tabela campanha_month_yes"
      ],
      "metadata": {
        "id": "xF2xxdTRnLCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela campanha_conversão"
      ],
      "metadata": {
        "id": "72-QEhtznkjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Supondo que 'bank_ad_campanha' é o DataFrame que contém os dados\n",
        "df = bank_ad_campanha\n",
        "\n",
        "# Agrupar por month e contar a quantidade de vezes que aparece cada month\n",
        "grouped_df = df.groupBy('month').agg(F.count('month').alias('count'))\n",
        "\n",
        "# Calcular a taxa de conversão anterior (poutcome = success)\n",
        "conversao_anterio_df = df.filter(df['poutcome'] == 'success') \\\n",
        "    .groupBy('month').agg((F.count('poutcome') / df.filter(df['poutcome'].isNotNull()).count() * 100).alias('conversão_anterio'))\n",
        "\n",
        "# Calcular a taxa de conversão atual (y = yes)\n",
        "conversao_atual_df = df.filter(df['y'] == 'yes') \\\n",
        "    .groupBy('month').agg((F.count('y') / df.filter(df['y'].isNotNull()).count() * 100).alias('conversão_atual'))\n",
        "\n",
        "# Juntar os DataFrames em um único DataFrame\n",
        "result_df = grouped_df.join(conversao_anterio_df, 'month', 'left') \\\n",
        "    .join(conversao_atual_df, 'month', 'left')\n",
        "\n",
        "# Ordenar o DataFrame resultante por mês (de janeiro a dezembro)\n",
        "meses_ordenados = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
        "result_df = result_df.withColumn(\"month\", F.substring(\"month\", 0, 3)) \\\n",
        "    .withColumn(\"month\", F.when(F.col(\"month\").isin(meses_ordenados), F.col(\"month\"))) \\\n",
        "    .orderBy(F.when(F.col(\"month\") == \"jan\", 1)\n",
        "              .when(F.col(\"month\") == \"feb\", 2)\n",
        "              .when(F.col(\"month\") == \"mar\", 3)\n",
        "              .when(F.col(\"month\") == \"apr\", 4)\n",
        "              .when(F.col(\"month\") == \"may\", 5)\n",
        "              .when(F.col(\"month\") == \"jun\", 6)\n",
        "              .when(F.col(\"month\") == \"jul\", 7)\n",
        "              .when(F.col(\"month\") == \"aug\", 8)\n",
        "              .when(F.col(\"month\") == \"sep\", 9)\n",
        "              .when(F.col(\"month\") == \"oct\", 10)\n",
        "              .when(F.col(\"month\") == \"nov\", 11)\n",
        "              .when(F.col(\"month\") == \"dec\", 12)\n",
        "              .otherwise(F.lit(99)))\n",
        "\n",
        "# Exibir o resultado\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgvxZXtdsWOy",
        "outputId": "e2ee0427-ec3f-4aaf-e769-78b7d21b5b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------------------+-------------------+\n",
            "|month|count|  conversão_anterio|    conversão_atual|\n",
            "+-----+-----+-------------------+-------------------+\n",
            "|  mar|  546|0.19665922113236864| 0.6700980868214043|\n",
            "|  apr| 2632| 0.2694959696999126|  1.308633582596873|\n",
            "|  may|13769| 0.5584150723511703| 2.1511119743614646|\n",
            "|  jun| 5318|0.34961639312421094| 1.3571914149752355|\n",
            "|  jul| 7174| 0.2622122948431582| 1.5757016606778675|\n",
            "|  aug| 6178|0.49771778187821697|  1.590269010391376|\n",
            "|  sep|  570|0.35932795959988345| 0.6215402544430416|\n",
            "|  oct|  718| 0.3301932601728659| 0.7647858599592114|\n",
            "|  nov| 4101| 0.4006021171214917| 1.0100029134699426|\n",
            "|  dec|  182|0.10925512285131592|0.21608235408371368|\n",
            "+-----+-----+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Supondo que 'bank_ad_campanha' é o DataFrame que contém os dados\n",
        "df = bank_ad_campanha\n",
        "\n",
        "# Agrupar por month e contar a quantidade de vezes que aparece cada month\n",
        "grouped_df = df.groupBy('month').agg(F.count('month').alias('count'))\n",
        "\n",
        "# Contar a quantidade de vezes que poutcome = success aparece em cada month\n",
        "count_anterio_df = df.filter(df['poutcome'] == 'success') \\\n",
        "    .groupBy('month').agg(F.count('poutcome').alias('count_anterio'))\n",
        "\n",
        "# Contar a quantidade de vezes que y = yes aparece em cada month\n",
        "count_atual_df = df.filter(df['y'] == 'yes') \\\n",
        "    .groupBy('month').agg(F.count('y').alias('count_atual'))\n",
        "\n",
        "# Contar a quantidade total de linhas em poutcome e y\n",
        "total_poutcome = df.filter(df['poutcome'].isNotNull()).count()\n",
        "total_y = df.filter(df['y'].isNotNull()).count()\n",
        "\n",
        "# Calcular a taxa de conversão anterior (poutcome = success)\n",
        "conversao_anterio_df = count_anterio_df.withColumn('conversão_anterio', (F.col('count_anterio') / total_poutcome * 100))\n",
        "\n",
        "# Calcular a taxa de conversão atual (y = yes)\n",
        "conversao_atual_df = count_atual_df.withColumn('conversão_atual', (F.col('count_atual') / total_y * 100))\n",
        "\n",
        "# Juntar todos os DataFrames em um único DataFrame\n",
        "result_df = grouped_df.join(count_anterio_df, 'month', 'left') \\\n",
        "    .join(count_atual_df, 'month', 'left') \\\n",
        "    .join(conversao_anterio_df, 'month', 'left') \\\n",
        "    .join(conversao_atual_df, 'month', 'left')\n",
        "\n",
        "# Ordenar o DataFrame resultante por mês (de janeiro a dezembro)\n",
        "meses_ordenados = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
        "result_df = result_df.withColumn(\"month\", F.substring(\"month\", 0, 3)) \\\n",
        "    .withColumn(\"month\", F.when(F.col(\"month\").isin(meses_ordenados), F.col(\"month\"))) \\\n",
        "    .orderBy(F.when(F.col(\"month\") == \"jan\", 1)\n",
        "              .when(F.col(\"month\") == \"feb\", 2)\n",
        "              .when(F.col(\"month\") == \"mar\", 3)\n",
        "              .when(F.col(\"month\") == \"apr\", 4)\n",
        "              .when(F.col(\"month\") == \"may\", 5)\n",
        "              .when(F.col(\"month\") == \"jun\", 6)\n",
        "              .when(F.col(\"month\") == \"jul\", 7)\n",
        "              .when(F.col(\"month\") == \"aug\", 8)\n",
        "              .when(F.col(\"month\") == \"sep\", 9)\n",
        "              .when(F.col(\"month\") == \"oct\", 10)\n",
        "              .when(F.col(\"month\") == \"nov\", 11)\n",
        "              .when(F.col(\"month\") == \"dec\", 12)\n",
        "              .otherwise(F.lit(99)))\n",
        "\n",
        "# Exibir o resultado\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I--5wfmMz9qi",
        "outputId": "f5c39844-5b20-4a05-efaf-148cb364c9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------------+-----------+-------------+-------------------+-----------+-------------------+\n",
            "|month|count|count_anterio|count_atual|count_anterio|  conversão_anterio|count_atual|    conversão_atual|\n",
            "+-----+-----+-------------+-----------+-------------+-------------------+-----------+-------------------+\n",
            "|  mar|  546|           81|        276|           81|0.19665922113236864|        276| 0.6700980868214043|\n",
            "|  apr| 2632|          111|        539|          111| 0.2694959696999126|        539|  1.308633582596873|\n",
            "|  may|13769|          230|        886|          230| 0.5584150723511703|        886| 2.1511119743614646|\n",
            "|  jun| 5318|          144|        559|          144|0.34961639312421094|        559| 1.3571914149752355|\n",
            "|  jul| 7174|          108|        649|          108| 0.2622122948431582|        649| 1.5757016606778675|\n",
            "|  aug| 6178|          205|        655|          205|0.49771778187821697|        655|  1.590269010391376|\n",
            "|  sep|  570|          148|        256|          148|0.35932795959988345|        256| 0.6215402544430416|\n",
            "|  oct|  718|          136|        315|          136| 0.3301932601728659|        315| 0.7647858599592114|\n",
            "|  nov| 4101|          165|        416|          165| 0.4006021171214917|        416| 1.0100029134699426|\n",
            "|  dec|  182|           45|         89|           45|0.10925512285131592|         89|0.21608235408371368|\n",
            "+-----+-----+-------------+-----------+-------------+-------------------+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renomear as colunas\n",
        "bank_ad_campanha = bank_ad_campanha.withColumnRenamed(\"emp.var.rate\", \"emp_var_rate\") \\\n",
        "           .withColumnRenamed(\"cons.price.idx\", \"cons_price_idx\") \\\n",
        "           .withColumnRenamed(\"cons.conf.idx\", \"cons_conf_idx\") \\\n",
        "           .withColumnRenamed(\"nr.employed\", \"nr_employed\")\n",
        "\n",
        "bank_ad_campanha.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsPVTtCw_EEp",
        "outputId": "b21bdbb5-f19f-40d2-aa18-4d06763f9f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|  y|\n",
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|      55|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     222|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     137|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     293|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     146|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     174|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     312|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     440|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     353|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "|telephone|  may|        mon|     195|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Supondo que 'bank_ad_campanha' é o DataFrame que contém os dados\n",
        "df = bank_ad_campanha\n",
        "\n",
        "# Filtrar os valores para 'total', 'positiva_anterior' (poutcome = success) e 'positiva_atual' (y = yes)\n",
        "total_df = df.groupBy().agg(F.lit('total').alias('campanha'),\n",
        "                            F.mean('emp_var_rate').alias('taxa_variacao_emprego_media'),\n",
        "                            F.mean('cons_price_idx').alias('indice_preco_consumidor_medio'),\n",
        "                            F.mean('cons_conf_idx').alias('indice_confianca_consumidor_medio'),\n",
        "                            F.mean('euribor3m').alias('eurbor3m_media'),\n",
        "                            F.mean('nr_employed').alias('n_funcionarios_medio'))\n",
        "\n",
        "positiva_anterior_df = df.filter(df['poutcome'] == 'success').groupBy().agg(F.lit('positiva_anterior').alias('campanha'),\n",
        "                                                                             F.mean('emp_var_rate').alias('taxa_variacao_emprego_media'),\n",
        "                                                                             F.mean('cons_price_idx').alias('indice_preco_consumidor_medio'),\n",
        "                                                                             F.mean('cons_conf_idx').alias('indice_confianca_consumidor_medio'),\n",
        "                                                                             F.mean('euribor3m').alias('eurbor3m_media'),\n",
        "                                                                             F.mean('nr_employed').alias('n_funcionarios_medio'))\n",
        "\n",
        "positiva_atual_df = df.filter(df['y'] == 'yes').groupBy().agg(F.lit('positiva_atual').alias('campanha'),\n",
        "                                                              F.mean('emp_var_rate').alias('taxa_variacao_emprego_media'),\n",
        "                                                              F.mean('cons_price_idx').alias('indice_preco_consumidor_medio'),\n",
        "                                                              F.mean('cons_conf_idx').alias('indice_confianca_consumidor_medio'),\n",
        "                                                              F.mean('euribor3m').alias('eurbor3m_media'),\n",
        "                                                              F.mean('nr_employed').alias('n_funcionarios_medio'))\n",
        "\n",
        "# Unir os DataFrames em um único DataFrame\n",
        "result_df = total_df.union(positiva_anterior_df).union(positiva_atual_df)\n",
        "\n",
        "# Exibir o resultado\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T67qPfC1Chaf",
        "outputId": "ba623f6b-7d88-4393-e6eb-edb58e69b102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------------+-----------------------------+---------------------------------+------------------+--------------------+\n",
            "|         campanha|taxa_variacao_emprego_media|indice_preco_consumidor_medio|indice_confianca_consumidor_medio|    eurbor3m_media|n_funcionarios_medio|\n",
            "+-----------------+---------------------------+-----------------------------+---------------------------------+------------------+--------------------+\n",
            "|            total|        0.08188550063146356|            93.57566436825361|               -40.50260027191408|3.6212908128580406|   5167.035910943787|\n",
            "|positiva_anterior|        -2.0912600145666183|            93.33435542607359|               -38.38856518572522|0.9965630007283336|   5030.622432629189|\n",
            "|   positiva_atual|        -1.2334482758620777|             93.3543859913807|              -39.789784482758215|2.1231351293103584|   5095.115991379242|\n",
            "+-----------------+---------------------------+-----------------------------+---------------------------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Criar uma sessão no Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Supondo que 'bank_ad_campanha' é o DataFrame que contém os dados\n",
        "df = bank_ad_campanha\n",
        "\n",
        "# Agrupar por month e contar a quantidade de vezes que aparece cada month\n",
        "grouped_df = df.groupBy('month').agg(\n",
        "    F.count('month').alias('count'),\n",
        "    F.count(F.when(df['poutcome'] == 'success', True)).alias('count_anterio'),\n",
        "    F.count(F.when(df['y'] == 'yes', True)).alias('count_atual')\n",
        ")\n",
        "\n",
        "# Calcular a quantidade total de linhas em poutcome e y\n",
        "total_poutcome = df.filter(df['poutcome'].isNotNull()).count()\n",
        "total_y = df.filter(df['y'].isNotNull()).count()\n",
        "\n",
        "# Calcular a taxa de conversão anterior (poutcome = success)\n",
        "conversao_anterio_df = grouped_df.withColumn('conversão_anterio', (F.col('count_anterio') / total_poutcome * 100))\n",
        "\n",
        "# Calcular a taxa de conversão atual (y = yes)\n",
        "conversao_atual_df = conversao_anterio_df.withColumn('conversão_atual', (F.col('count_atual') / total_y * 100))\n",
        "\n",
        "# Juntar os DataFrames em um único DataFrame\n",
        "result_df = conversao_atual_df.drop('count_anterio', 'count_atual')\n",
        "\n",
        "# Ordenar o DataFrame resultante por mês (de janeiro a dezembro)\n",
        "meses_ordenados = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
        "result_df = result_df.withColumn(\"month_num\", F.month(F.from_unixtime(F.unix_timestamp(\"month\", \"MMM\"))))\n",
        "windowSpec = Window.orderBy(F.when(F.col(\"month_num\").isin(meses_ordenados), F.col(\"month_num\")))\n",
        "result_df = result_df.withColumn(\"month_num\", F.row_number().over(windowSpec)).drop(\"month_num\")\n",
        "result_df = result_df.orderBy(F.col(\"month_num\"))\n",
        "\n",
        "# Exibir o resultado\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM5ORcG5cUMU",
        "outputId": "19362a96-3171-45d8-e660-8705a1ebbb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------------------+-------------------+\n",
            "|month|count|  conversão_anterio|    conversão_atual|\n",
            "+-----+-----+-------------------+-------------------+\n",
            "|  jun| 5318|0.34961639312421094| 1.3571914149752355|\n",
            "|  aug| 6178|0.49771778187821697|  1.590269010391376|\n",
            "|  may|13769| 0.5584150723511703| 2.1511119743614646|\n",
            "|  sep|  570|0.35932795959988345| 0.6215402544430416|\n",
            "|  mar|  546|0.19665922113236864| 0.6700980868214043|\n",
            "|  oct|  718| 0.3301932601728659| 0.7647858599592114|\n",
            "|  jul| 7174| 0.2622122948431582| 1.5757016606778675|\n",
            "|  nov| 4101| 0.4006021171214917| 1.0100029134699426|\n",
            "|  apr| 2632| 0.2694959696999126|  1.308633582596873|\n",
            "|  dec|  182|0.10925512285131592|0.21608235408371368|\n",
            "+-----+-----+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}